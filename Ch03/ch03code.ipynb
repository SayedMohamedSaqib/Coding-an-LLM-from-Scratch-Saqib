{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acdc515",
   "metadata": {},
   "source": [
    "## Simple Self attention mechanism ¶\n",
    "The goal of self attention is to compute context vectors which are an enriched embeddeding that combines information from all other element. In other words \n",
    "for every token we try and create a better represenetation by allowing the an input in a sequence to interact and weigh in the importance of all other positons in the same sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34e71e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "#First we compute intermediate values or attention scores\n",
    "import torch\n",
    "torch.manual_seed(123)\n",
    "import torch\n",
    "inputs = torch.tensor(\n",
    " [[0.43, 0.15, 0.89], # Your (x^1)\n",
    " [0.55, 0.87, 0.66], # journey (x^2)\n",
    " [0.57, 0.85, 0.64], # starts (x^3)\n",
    " [0.22, 0.58, 0.33], # with (x^4)\n",
    " [0.77, 0.25, 0.10], # one (x^5)\n",
    " [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb40935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "#Then we normalize the weights \n",
    "attn_weights_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33028ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "#It is advisable to use softmax function for normalization which is better at managing extreme values and offers more favorable gradient properties during training\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim = 0)\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83a772",
   "metadata": {},
   "source": [
    "The softmax function ensures that the attention weights are always positive. This makes the output interpretable as probabilities or relative importance,\n",
    "where higher weights indicate greater importance.\n",
    "Note - that this naive softmax implementation (softmax_naive) may encounter\n",
    "numerical instability problems, such as overflow and underflow, when dealing with\n",
    "large or small input values. Therefore, in practice, it’s advisable to use the PyTorch\n",
    "implementation of softmax,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b59903",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb73868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#PyTorch implementation of softmax\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bedab2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "#context vector z(2) is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2+= attn_weights_2[i] * x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c92ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Computing attention scores for all inputs\n",
    "attn_scores = torch.empty(6,6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j,x_j in enumerate(inputs): \n",
    "        attn_scores[i,j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f1988",
   "metadata": {},
   "source": [
    "When computing the preceding attention score tensor, we used for loops in\n",
    "Python. However, for loops are generally slow, and we can achieve the same results\n",
    "using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1fe9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de91b7d",
   "metadata": {},
   "source": [
    "The dim parameter in functions like torch.softmax\n",
    "specifies the dimension of the input tensor along which the function will be com\u0002puted. By setting dim=-1, we are instructing the softmax function to apply the nor\u0002malization along the last dimension of the attn_scores tensor. If attn_scores is a\n",
    "two-dimensional tensor (for example, with a shape of [rows, columns]), it will nor\u0002malize across the columns so that the values in each row (summing over the column\n",
    "dimension) sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7179a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "#Computing attention weights \n",
    "attn_weights = torch.softmax(attn_scores, dim = -1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bae172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25f448",
   "metadata": {},
   "source": [
    "## Implementing self-attention with trainable weights\n",
    "The most notable difference is the introduction of weight matrices that are\n",
    "updated during model training. These trainable weight matrices are crucial so that\n",
    "the model (specifically, the attention module inside the model) can learn to produce\n",
    "“good” context vectors. \n",
    "\n",
    "We will implement the self-attention mechanism step by step by introducing the\n",
    "three trainable weight matrices Wq, Wk, and Wv. These three matrices are used to\n",
    "project the embedded input tokens, x(i), into query, key, and value vectors, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea4aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start here by computing only one context vector, z(2), for illustration purposes.\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f97d3",
   "metadata": {},
   "source": [
    "We set requires_grad=False to reduce clutter in the outputs, but if we were to use\n",
    "the weight matrices for model training, we would set requires_grad=True to update\n",
    "these matrices during model training. \n",
    "\n",
    "Next we compute Query, Key and Value Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bad3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b6825",
   "metadata": {},
   "source": [
    "Even though our temporary goal is only to compute the one context vector, z(2), we still\n",
    "require the key and value vectors for all input elements as they are involved in computing the attention weights with respect to the query q(2).\n",
    "We can obtain all keys and values via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b5306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc258f",
   "metadata": {},
   "source": [
    "The second step is to compute the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e482a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(key_2)\n",
    "print(attn_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eabfb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "#Generalizing to compute all attention scores\n",
    "attn_scores_2 = query_2 @ keys.T \n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2bad31",
   "metadata": {},
   "source": [
    "We compute the attention weights by scaling the attention scores and\n",
    "using the softmax function. However, now we scale the attention scores by dividing\n",
    "them by the square root of the embedding dimension of the keys (taking the square\n",
    "root is mathematically the same as exponentiating by 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "594b2045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k ** 0.5, dim = -1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff1d12",
   "metadata": {},
   "source": [
    "Similar to when we computed the context vector as a weighted sum over the input vectors, we now compute the context vector as a weighted sum over the\n",
    "value vectors. Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. Also as before, we can use matrix multiplication to obtain the output in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0006cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50feca7",
   "metadata": {},
   "source": [
    "## Implementing a compact self-attention Python class\n",
    "\n",
    "At this point, we have gone through a lot of steps to compute the self-attention outputs. We did so mainly for illustration purposes so we could go through one step at a\n",
    "time. In practice, with the LLM implementation in the next chapter in mind, it is\n",
    "helpful to organize this code into a Python class, as shown below\n",
    "\n",
    "What is nn.Module?\n",
    "\n",
    " .nn.Module is the base class for all neural network components in PyTorch.\n",
    "\n",
    " .Every layer (nn.Linear, nn.Conv2d, nn.LSTM, etc.) is a subclass of nn.Module.\n",
    "\n",
    " .Whole models (like GPT, ResNet, etc.) are also subclasses of nn.Module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80b72f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d46d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim = -1)\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a06aef41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa278c8",
   "metadata": {},
   "source": [
    "We can improve the SelfAttention_v1 implementation further by utilizing\n",
    "PyTorch’s nn.Linear layers, which effectively perform matrix multiplication when\n",
    "the bias units are disabled. Additionally, a significant advantage of using nn.Linear\n",
    "instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear\n",
    "has an optimized weight initialization scheme, contributing to more stable and\n",
    "effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a81ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78750e7",
   "metadata": {},
   "source": [
    "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\n",
    "they use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "sophisticated weight initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3fdf039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eafcee",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "Note that nn.Linear in SelfAttention_v2 uses a different weight initialization\n",
    "scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1,\n",
    "which causes both mechanisms to produce different results. To check that both\n",
    "implementations, SelfAttention_v1 and SelfAttention_v2, are otherwise similar, we can transfer the weight matrices from a SelfAttention_v2 object to a SelfAttention_v1, such that both objects then produce the same results.\n",
    "Your task is to correctly assign the weights from an instance of SelfAttention_v2\n",
    "to an instance of SelfAttention_v1. To do this, you need to understand the relationship between the weights in both versions. (Hint: nn.Linear stores the weight\n",
    "matrix in a transposed form.) After the assignment, you should observe that both\n",
    "instances produce the same outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83c7a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Collecting weight instances from SelfAttention_v2\n",
    "sa_v1 = SelfAttention_v1(d_in , d_out)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "with torch.no_grad():  # we don’t need gradients for this\n",
    "    sa_v1.W_query.copy_(sa_v2.W_query.weight.T)\n",
    "    sa_v1.W_key.copy_(sa_v2.W_key.weight.T)\n",
    "    sa_v1.W_value.copy_(sa_v2.W_value.weight.T)\n",
    "\n",
    "out_v2 = sa_v2(inputs)\n",
    "out_v1 = sa_v1(inputs)\n",
    "\n",
    "print(torch.allclose(out_v1, out_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9368ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1839, 0.0178],\n",
      "        [0.1815, 0.0205],\n",
      "        [0.1818, 0.0202],\n",
      "        [0.1826, 0.0191],\n",
      "        [0.1875, 0.0144],\n",
      "        [0.1799, 0.0218]], grad_fn=<MmBackward0>) \n",
      " \n",
      " tensor([[0.1839, 0.0178],\n",
      "        [0.1815, 0.0205],\n",
      "        [0.1818, 0.0202],\n",
      "        [0.1826, 0.0191],\n",
      "        [0.1875, 0.0144],\n",
      "        [0.1799, 0.0218]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out_v1, \"\\n \\n\", out_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "526d52fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2061,  0.3977],\n",
      "        [-0.2032,  0.3992],\n",
      "        [-0.2032,  0.3991],\n",
      "        [-0.2039,  0.3977],\n",
      "        [-0.2044,  0.3967],\n",
      "        [-0.2035,  0.3986]], grad_fn=<MmBackward0>) \n",
      " \n",
      " tensor([[-0.2061,  0.3977],\n",
      "        [-0.2032,  0.3992],\n",
      "        [-0.2032,  0.3991],\n",
      "        [-0.2039,  0.3977],\n",
      "        [-0.2044,  0.3967],\n",
      "        [-0.2035,  0.3986]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#alternative method, here .weight is an attribute to extract weight values from the matrix, .values is the entire linear object\n",
    "sa_v1 = SelfAttention_v1(d_in , d_out)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_key   = torch.nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\n",
    "\n",
    "out_v2 = sa_v2(inputs)\n",
    "out_v1 = sa_v1(inputs)\n",
    "\n",
    "print(out_v1, \"\\n \\n\", out_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592feda3",
   "metadata": {},
   "source": [
    "##  Hiding future words with causal attention\n",
    "Causal attention, also known as masked attention, is a specialized form of selfattention. It restricts a model to only consider previous and current inputs in a sequence\n",
    "when processing any given token when computing attention scores. This is in contrast\n",
    "to the standard self-attention mechanism, which allows access to the entire input sequence at once\n",
    "\n",
    "To achieve this in GPT-like LLMs, for each token processed, we mask out\n",
    "the future tokens, which come after the current token in the input text. We mask out the attention weights above the diagonal, and we normalize the nonmasked attention weights such that the attention weights sum to 1 in\n",
    "each row\n",
    "\n",
    "Our next step is to implement the causal attention mask in code. To implement the\n",
    "steps to apply a causal attention mask to obtain the masked attention weights, let’s work with the attention scores and weights from the previous section to code the causal attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61cbf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8071c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1574, 0.1694, 0.1692, 0.1687, 0.1649, 0.1703],\n",
      "        [0.1588, 0.1672, 0.1667, 0.1720, 0.1577, 0.1775],\n",
      "        [0.1591, 0.1672, 0.1666, 0.1719, 0.1579, 0.1773],\n",
      "        [0.1626, 0.1667, 0.1664, 0.1700, 0.1609, 0.1734],\n",
      "        [0.1651, 0.1664, 0.1662, 0.1686, 0.1630, 0.1706],\n",
      "        [0.1602, 0.1671, 0.1666, 0.1712, 0.1591, 0.1758]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we compute the attention weights using the softmax function as we have done previously:\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim = -1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec203fc",
   "metadata": {},
   "source": [
    "We can implement the second step using PyTorch’s tril function to create a mask\n",
    "where the values above the diagonal are zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2fd8fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6747ee1",
   "metadata": {},
   "source": [
    "Now, we can multiply this mask with the attention weights to zero-out the values above\n",
    "the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "960b6987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1574, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1588, 0.1672, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1591, 0.1672, 0.1666, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1626, 0.1667, 0.1664, 0.1700, 0.0000, 0.0000],\n",
      "        [0.1651, 0.1664, 0.1662, 0.1686, 0.1630, 0.0000],\n",
      "        [0.1602, 0.1671, 0.1666, 0.1712, 0.1591, 0.1758]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c23dac",
   "metadata": {},
   "source": [
    "The third step is to renormalize the attention weights to sum up to 1 again in each\n",
    "row. We can achieve this by dividing each element in each row by the sum in each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2d8b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4871, 0.5129, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3227, 0.3392, 0.3381, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2442, 0.2505, 0.2499, 0.2554, 0.0000, 0.0000],\n",
      "        [0.1991, 0.2006, 0.2004, 0.2033, 0.1966, 0.0000],\n",
      "        [0.1602, 0.1671, 0.1666, 0.1712, 0.1591, 0.1758]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rows_sum = masked_simple.sum(dim = -1, keepdim=True)\n",
    "masked_simple_norm = masked_simple/rows_sum\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affa37f",
   "metadata": {},
   "source": [
    " While we could wrap up our implementation of causal attention at this point, we can\n",
    " still improve it. Let’s take a mathematical property of the softmax function and imple\n",
    "ment the computation of the masked attention weights more efficiently in fewer steps,\n",
    "\n",
    "The softmax function converts its inputs into a probability distribution. When nega\n",
    "tive infinity values (-∞) are present in a row, the softmax function treats them as zero\n",
    " probability. (Mathematically, this is because e–∞ approaches 0.)\n",
    " We can implement this more efficient masking “trick” by creating a mask with 1s\n",
    " above the diagonal and then replacing these 1s with negative infinity (-inf) values:\n",
    "\n",
    " We effectively avoid the renormalization step where we divide by row_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3303e163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0555,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.1005, -0.0276,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.0984, -0.0277, -0.0325,    -inf,    -inf,    -inf],\n",
      "        [-0.0603, -0.0247, -0.0278,  0.0029,    -inf,    -inf],\n",
      "        [-0.0328, -0.0220, -0.0238, -0.0033, -0.0507,    -inf],\n",
      "        [-0.0840, -0.0240, -0.0281,  0.0099, -0.0935,  0.0473]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "552aa90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4871, 0.5129, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3227, 0.3392, 0.3381, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2442, 0.2505, 0.2499, 0.2554, 0.0000, 0.0000],\n",
      "        [0.1991, 0.2006, 0.2004, 0.2033, 0.1966, 0.0000],\n",
      "        [0.1602, 0.1671, 0.1666, 0.1712, 0.1591, 0.1758]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying Softmax finally\n",
    "attn_weights = torch.softmax(masked/keys.shape[-1] ** 0.5, dim = -1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da16fa",
   "metadata": {},
   "source": [
    "## Masking additional attention weights with dropout\n",
    "This method helps prevent overfitting by ensuring that a model does not become overly reliant on any spe\n",
    "cific set of hidden layer units. It’s important to emphasize that dropout is only used\n",
    " during training and is disabled afterward.\n",
    " In the transformer architecture, including models like GPT, dropout in the atten\n",
    "tion mechanism is typically applied at two specific times: after calculating the atten\n",
    "tion weights or after applying the attention weights to the value vectors. Here we will\n",
    " apply the dropout mask after computing the attention weights, because it’s the more common variant in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50eb342a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#Example implementation\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "439a48f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6454, 0.6784, 0.6762, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5009, 0.4998, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4013, 0.0000, 0.4066, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3342, 0.3333, 0.3424, 0.3182, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Applying to our attention weights\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f1178",
   "metadata": {},
   "source": [
    "Before we begin, let’s ensure that the code can handle batches consisting of\n",
    " more than one input so that the CausalAttention class supports the batch outputs\n",
    " produced by the data loader we implemented in chapter 2.\n",
    " For simplicity, to simulate such batch inputs, we duplicate the input text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1edfb571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "#.stack combines a sequence of tensors by adding a new dimension\n",
    "batch = torch.stack((inputs,inputs) , dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66bf0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing the causal self attention class \n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    def forward(self,x):\n",
    "        b,num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        attn_scores.masked_fill_(self.mask.bool() [:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5aef22",
   "metadata": {},
   "source": [
    "We can use the CausalAttention class as follows, similar to SelfAttention\n",
    " previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "745feb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb628c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4821,  0.4336],\n",
      "         [-0.5368,  0.5483],\n",
      "         [-0.5545,  0.5886],\n",
      "         [-0.4937,  0.5311],\n",
      "         [-0.4589,  0.5169],\n",
      "         [-0.4479,  0.4971]],\n",
      "\n",
      "        [[-0.4821,  0.4336],\n",
      "         [-0.5368,  0.5483],\n",
      "         [-0.5545,  0.5886],\n",
      "         [-0.4937,  0.5311],\n",
      "         [-0.4589,  0.5169],\n",
      "         [-0.4479,  0.4971]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80d5ea",
   "metadata": {},
   "source": [
    " A wrapper class to implement multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3958a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        #Wrapping python list into a PyTorch List, to specify modules and update their parameters\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range (num_heads)]\n",
    "        )\n",
    "        \n",
    "        #run list comprehension through each head, and concatenate along embedding dimension\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3de7733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "#Testing our Multi Head Attention Wrapper\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in , d_out = 3 , 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad521a",
   "metadata": {},
   "source": [
    "Exercise 3.2 \n",
    "\n",
    "Returning two-dimensional embedding vectors \n",
    "Change the input arguments for the MultiHeadAttentionWrapper(..., num_\n",
    " heads=2) call such that the output context vectors are two-dimensional instead of\n",
    " four dimensional while keeping the setting num_heads=2. Hint: You don’t have to\n",
    " modify the class implementation; you just have to change one of the other input\n",
    " arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b184350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0189, 0.2729],\n",
      "         [0.2181, 0.3037],\n",
      "         [0.2804, 0.3125],\n",
      "         [0.2830, 0.2793],\n",
      "         [0.2476, 0.2541],\n",
      "         [0.2748, 0.2513]],\n",
      "\n",
      "        [[0.0189, 0.2729],\n",
      "         [0.2181, 0.3037],\n",
      "         [0.2804, 0.3125],\n",
      "         [0.2830, 0.2793],\n",
      "         [0.2476, 0.2541],\n",
      "         [0.2748, 0.2513]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "d_in,d_out = 3,1\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdc9c9",
   "metadata": {},
   "source": [
    "##  Implementing multi-head attention with weight splits\n",
    "\n",
    "The following MultiHeadAttention class integrates the multi-head functionality within a single class.\n",
    "It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbe90c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads #Projection dimension reduced to match output dimension\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) #Linear layer to combine outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\" , torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self , x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores/ keys.shape[-1] ** 0.5, dim = -1)    \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        \n",
    "        context_vec = context_vec.contiguous().view(b , num_tokens, self.d_out)\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec) \n",
    "        return context_vec  \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1095fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Demonstration\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "                   \n",
    "                    [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c1eb0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "#perform a batched matrix multiplication between the tensor itself and a view of the tensor where we transposed the last two dimensions, num_tokens and head_dim:\n",
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5400278",
   "metadata": {},
   "source": [
    "The MultiHeadAttention class can be used similar to the SelfAttention and\n",
    " CausalAttention classes we implemented earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1cfbfa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising a class with similar attention heads and input/output dimensions as smallest GPT-2 Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
